{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import collections\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "from matplotlib import pyplot as  plt\n",
    "from scipy.special import erfinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = np.load('one_hot_seqs.npy')\n",
    "\n",
    "def id2one_hot(ids):\n",
    "    res = np.zeros((ids.shape[0], ids.shape[1], 21))\n",
    "    for i in range(ids.shape[0]):\n",
    "        for j in range(ids.shape[1]):\n",
    "            res[i, j, ids[i, j]] = 1\n",
    "    return res\n",
    "\n",
    "train = np.array(id2one_hot(np.load('id_seq.npy')))\n",
    "varient_onehot = np.load('variant_onehot.npy')\n",
    "varients = pd.read_csv('varient.csv')\n",
    "\n",
    "#params\n",
    "logit_sigma = 4.0\n",
    "logit_p = 0.01\n",
    "logit_mu = np.sqrt(2.0) * logit_sigma * erfinv(2.0 * logit_p - 1.0)\n",
    "latent_dim = 30\n",
    "seq_len = 82\n",
    "alp_len = 21\n",
    "sigma_init = -5\n",
    "def KLD_diag_gaussians(mu, log_sigma, prior_mu, prior_log_sigma):\n",
    "    return prior_log_sigma - log_sigma + 0.5 * (tf.exp(2. * log_sigma) + tf.square(mu - prior_mu)) * tf.exp(-2.0 * prior_log_sigma) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceLayer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def Sampler(self, mu, sigma):\n",
    "        eps = tf.random.normal(sigma.shape)\n",
    "        return mu + eps * (tf.exp(sigma)**0.5)    \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.wmu = self.add_variable(name='wmu',shape=[input_shape[-1], self.units],\n",
    "                                    initializer=tf.random_normal_initializer(0, tf.sqrt(2.0/(input_shape[-1] + self.units))))\n",
    "        self.wsigma = self.add_variable(name='wsigma',shape=[input_shape[-1], self.units], initializer=tf.constant_initializer(sigma_init))\n",
    "        self.bmu = self.add_variable(name='bmu',shape=[self.units], initializer=tf.constant_initializer(0.1))\n",
    "        self.bsigma = self.add_variable(name='bsigma',shape=[self.units], initializer=tf.constant_initializer(sigma_init))\n",
    "    \n",
    "    def KLD_params(self):\n",
    "        res = - tf.reduce_mean(KLD_diag_gaussians(self.wmu, self.wsigma, 0, 0.))\n",
    "        res -= tf.reduce_mean(KLD_diag_gaussians(self.bmu, self.bsigma, 0, 0.)) \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def call(self, inputs, var=True):     \n",
    "        w = self.Sampler(self.wmu, self.wsigma)\n",
    "        b = self.Sampler(self.bmu, self.bsigma)\n",
    "        output = tf.matmul(inputs, w) + b\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(layers.Layer):\n",
    "    def __init__(self, units, seq_len, n_pat = 4):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.seq_len = seq_len\n",
    "        self.n_pat = n_pat\n",
    "        \n",
    "        \n",
    "    def Sampler(self, mu, sigma):\n",
    "        eps = tf.random.normal(sigma.shape)\n",
    "        return mu + eps * (tf.exp(sigma)**0.5)    \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.inputshape = input_shape\n",
    "        self.w_out_mu = self.add_variable(name='w_out_mu',shape=[input_shape[-1] * self.seq_len, self.units], \n",
    "                                          initializer=tf.random_normal_initializer(0, tf.sqrt(2.0/(input_shape[-1] * self.seq_len+self.units))))\n",
    "        self.w_out_sigma = self.add_variable(name='w_out_sigma',shape=[input_shape[-1] * self.seq_len, self.units], \n",
    "                                             initializer=tf.constant_initializer(sigma_init))\n",
    "        \n",
    "        self.w_conv_mu = self.add_variable(name='w_conv_mu',shape=[self.units, alp_len], \n",
    "                                           initializer=tf.random_normal_initializer(0, tf.sqrt(2.0/(self.units*alp_len))))\n",
    "        self.w_conv_sigma = self.add_variable(name='w_conv_sigma',shape=[self.units, alp_len], \n",
    "                                              initializer=tf.constant_initializer(sigma_init))\n",
    "        \n",
    "        self.w_scale_mu = self.add_variable(name='w_scale_mu',shape=[int(input_shape[-1] / self.n_pat), self.seq_len], \n",
    "                                            initializer=tf.random_normal_initializer(0, tf.sqrt(2.0/(int(input_shape[-1] / self.n_pat)*self.seq_len))))\n",
    "        self.w_scale_sigma = self.add_variable(name='w_scale_sigma',shape=[int(input_shape[-1] / self.n_pat), self.seq_len], \n",
    "                                               initializer=tf.constant_initializer(sigma_init))\n",
    "        \n",
    "        self.bmu = self.add_variable(name='bmu',shape=[self.seq_len * alp_len], \n",
    "                                     initializer=tf.random_normal_initializer(0.1))\n",
    "        self.bsigma = self.add_variable(name='bsigma',shape=[self.seq_len * alp_len], \n",
    "                                        initializer=tf.constant_initializer(sigma_init))\n",
    "        self.psw_mu = self.add_variable(name='psw_mu', shape=[1], initializer=tf.constant_initializer(1))\n",
    "        self.psw_sigma = self.add_variable(name='psw_sigma', shape=[1], initializer=tf.constant_initializer(sigma_init))\n",
    "    \n",
    "    \n",
    "    def FadeOut(self):\n",
    "        return tf.reduce_mean(KLD_diag_gaussians(self.w_scale_mu, self.w_scale_sigma, logit_mu, tf.math.log(logit_sigma)))\n",
    "    \n",
    "    \n",
    "    def KLD_params(self):\n",
    "        res = - tf.reduce_mean(KLD_diag_gaussians(self.w_conv_mu, self.w_conv_sigma, 0, 0))\n",
    "        res -= tf.reduce_mean(KLD_diag_gaussians(self.w_out_mu, self.w_out_sigma, 0, 0))\n",
    "        res -= tf.reduce_mean(KLD_diag_gaussians(self.bmu, self.bsigma, 0, 0))\n",
    "        return res - self.FadeOut()\n",
    "     \n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        w_out = self.Sampler(self.w_out_mu, self.w_out_sigma)\n",
    "        w_conv = self.Sampler(self.w_conv_mu, self.w_conv_sigma)\n",
    "        w_out = tf.matmul(w_out, w_conv)\n",
    "        w_scale = self.Sampler(self.w_scale_mu, self.w_scale_sigma)\n",
    "        w_scale = tf.tile(w_scale, (self.n_pat, 1))\n",
    "        w_scale = tf.expand_dims(w_scale, -1)\n",
    "        w_out = tf.reshape(w_out, (self.inputshape[-1], self.seq_len, alp_len))\n",
    "        w_out = tf.multiply(w_out, w_scale)\n",
    "        w_out = tf.reshape(w_out, (self.inputshape[-1], self.seq_len * alp_len))\n",
    "        \n",
    "        b = self.Sampler(self.bmu, self.bsigma)\n",
    "        output = tf.matmul(inputs, w_out) + b\n",
    "        psw = self.Sampler(self.psw_mu, self.psw_sigma)\n",
    "        return tf.multiply(output, tf.math.log(1.0 + tf.exp(psw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_Seq(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Deep_Seq, self).__init__()\n",
    "\n",
    "        \n",
    "        self.flat1 = layers.Flatten()\n",
    "        self.encoder1 = layers.Dense(1500)\n",
    "        \n",
    "        self.encoder2 = layers.Dense(1500)\n",
    "        \n",
    "        self.hmu = layers.Dense(latent_dim)\n",
    "        self.hsigma = layers.Dense(latent_dim)\n",
    "        \n",
    "        self.decoder1 = VarianceLayer(100)\n",
    "        self.decoder2 = VarianceLayer(500)\n",
    "\n",
    "        self.out = OutputLayer(10, seq_len)\n",
    "        \n",
    "        self.reshape = layers.Reshape((seq_len, alp_len))\n",
    "        \n",
    "        \n",
    "    def Encoder(self, x):\n",
    "        x = self.flat1(x)\n",
    "        x = self.encoder1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.encoder2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        mu = self.hmu(x)\n",
    "        sigma = self.hsigma(x)\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "    def Sampler(self, mu, sigma):\n",
    "        eps = tf.random.normal(sigma.shape)\n",
    "        return mu + eps * (tf.exp(sigma)**0.5)\n",
    "    \n",
    "    def Decoder(self, z):\n",
    "        z = self.decoder1(z)\n",
    "        z = tf.nn.relu(z)\n",
    "        \n",
    "        z = self.decoder2(z)\n",
    "        z = tf.nn.relu(z)\n",
    "\n",
    "        x_hat = self.out(z)\n",
    "        x_hat = self.reshape(x_hat)\n",
    "        \n",
    "        return x_hat\n",
    "    \n",
    "    def KLD_params(self):\n",
    "        return self.decoder1.KLD_params() + self.decoder2.KLD_params()+ self.out.KLD_params()\n",
    "        #\n",
    "    \n",
    "    def call(self, x):\n",
    "        mu, sigma = self.Encoder(x)\n",
    "        z = self.Sampler(mu, sigma)\n",
    "        x_hat = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer deep__seq is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-604df702a1ad>:11: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "step: 0, loss: 43.224094, spearman_corr: -0.030865 \n",
      "step: 500, loss: 38.361443, spearman_corr: 0.276990 \n",
      "step: 1000, loss: 34.722839, spearman_corr: 0.183478 \n",
      "step: 1500, loss: 31.053459, spearman_corr: 0.266248 \n",
      "step: 2000, loss: 27.416258, spearman_corr: 0.217077 \n",
      "step: 2500, loss: 23.931871, spearman_corr: 0.154710 \n",
      "step: 3000, loss: 20.442411, spearman_corr: 0.075791 \n",
      "step: 3500, loss: 17.129961, spearman_corr: 0.172258 \n",
      "step: 4000, loss: 14.043248, spearman_corr: 0.245259 \n",
      "step: 4500, loss: 11.429358, spearman_corr: 0.296177 \n",
      "step: 5000, loss: 9.609058, spearman_corr: 0.217949 \n",
      "step: 5500, loss: 8.329887, spearman_corr: 0.228614 \n",
      "step: 6000, loss: 7.393137, spearman_corr: 0.259248 \n",
      "step: 6500, loss: 6.854700, spearman_corr: 0.253355 \n",
      "step: 7000, loss: 6.267129, spearman_corr: 0.327531 \n",
      "step: 7500, loss: 6.067152, spearman_corr: 0.236268 \n",
      "step: 8000, loss: 5.728521, spearman_corr: 0.208459 \n",
      "step: 8500, loss: 5.576394, spearman_corr: 0.307008 \n",
      "step: 9000, loss: 5.299579, spearman_corr: 0.230314 \n",
      "step: 9500, loss: 5.446831, spearman_corr: 0.168631 \n",
      "step: 10000, loss: 5.055056, spearman_corr: 0.252004 \n",
      "step: 10500, loss: 5.022799, spearman_corr: 0.230458 \n",
      "step: 11000, loss: 4.756720, spearman_corr: 0.229764 \n",
      "step: 11500, loss: 4.896931, spearman_corr: 0.253311 \n",
      "step: 12000, loss: 4.987822, spearman_corr: 0.196543 \n",
      "step: 12500, loss: 4.702848, spearman_corr: 0.255854 \n",
      "step: 13000, loss: 4.686899, spearman_corr: 0.233293 \n",
      "step: 13500, loss: 4.412858, spearman_corr: 0.237367 \n",
      "step: 14000, loss: 4.250046, spearman_corr: 0.267325 \n",
      "step: 14500, loss: 4.266927, spearman_corr: 0.259994 \n",
      "step: 15000, loss: 4.089219, spearman_corr: 0.254643 \n",
      "step: 15500, loss: 4.050908, spearman_corr: 0.256771 \n",
      "step: 16000, loss: 4.276946, spearman_corr: 0.183707 \n",
      "step: 16500, loss: 4.158963, spearman_corr: 0.304945 \n",
      "step: 17000, loss: 4.563582, spearman_corr: 0.216931 \n",
      "step: 17500, loss: 4.415221, spearman_corr: 0.201389 \n",
      "step: 18000, loss: 3.930165, spearman_corr: 0.190849 \n",
      "step: 18500, loss: 3.959187, spearman_corr: 0.244939 \n",
      "step: 19000, loss: 3.817245, spearman_corr: 0.227011 \n",
      "step: 19500, loss: 4.454935, spearman_corr: 0.207236 \n",
      "step: 20000, loss: 3.897930, spearman_corr: 0.235213 \n",
      "step: 20500, loss: 3.783263, spearman_corr: 0.255281 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6610bedb909d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mce\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkl_div\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKLD_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    616\u001b[0m             \"ParameterServerStrategy and CentralStorageStrategy\")\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m       \u001b[0mapply_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform_unaggregated_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_prepare\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    863\u001b[0m       \u001b[0mapply_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py\u001b[0m in \u001b[0;36m_prepare_local\u001b[1;34m(self, var_device, var_dtype, apply_state)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0mbeta_2_power\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_2_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     lr = (apply_state[(var_device, var_dtype)]['lr_t'] *\n\u001b[1;32m--> 143\u001b[1;33m           (math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)))\n\u001b[0m\u001b[0;32m    144\u001b[0m     apply_state[(var_device, var_dtype)].update(\n\u001b[0;32m    145\u001b[0m         dict(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[1;34m(y, x)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1170\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m   \u001b[1;31m# Propagate func.__doc__ to the wrappers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m  10459\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10460\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10461\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m  10462\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Sub\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10463\u001b[0m         x, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae = Deep_Seq()#model\n",
    "batch_size = 512\n",
    "optimizer = tf.optimizers.Adam()\n",
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.repeat().shuffle(1000).batch(batch_size).prefetch(1)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "train_step = 100000\n",
    "display_step = 500\n",
    "losses = []\n",
    "corrs = []\n",
    "for step, batch_x in enumerate(train_data.take(train_step + 1)):\n",
    "   \n",
    "    with tf.GradientTape() as tape:\n",
    "        x_, mu, sigma = vae(batch_x)\n",
    "        kl_div = -0.5 * (2 * sigma + 1 - tf.square(mu) - tf.exp(2 * sigma))\n",
    "        kl_div = tf.reduce_mean(kl_div)\n",
    "        ce = tf.nn.softmax_cross_entropy_with_logits(batch_x, x_)\n",
    "        loss = tf.reduce_mean(ce) + kl_div - vae.KLD_params()\n",
    "    gradient = tape.gradient(loss, vae.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, vae.trainable_variables))\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        \n",
    "        pre, mu, sigma= vae(varient_onehot)\n",
    "        kl_div = tf.reduce_mean(-0.5 * (2 * sigma + 1 - tf.square(mu) - tf.exp(2 * sigma)), axis=1).numpy()\n",
    "        elbo = - tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(varient_onehot, pre), axis=1).numpy() - kl_div\n",
    "        pre_wt, mu, sigma= vae(train[0:1])\n",
    "        kl_div = tf.reduce_mean(-0.5 * (2 * sigma + 1 - tf.square(mu) - tf.exp(2 * sigma)), axis=1).numpy()\n",
    "        wt_elbo = - tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train[0:1], pre_wt)) - kl_div\n",
    "        \n",
    "        varients['pre'] = elbo - wt_elbo.numpy()[0]\n",
    "        corr = varients.corr('spearman').values[1][-1]\n",
    "        \n",
    "        corrs.append(corr)\n",
    "        losses.append(loss.numpy())\n",
    "        print(\"step: %i, loss: %f, spearman_corr: %f \"  % (step, loss, corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
